
@inproceedings{GreenwaldCorrelatedQLearning2003,
  address = {Washington, DC, USA},
  series = {ICML'03},
  title = {Correlated-{{Q Learning}}},
  isbn = {978-1-57735-189-4},
  abstract = {This paper introduces Correlated-Q (CE-Q) learning, a multiagent Q-learning algorithm based on the correlated equilibrium (CE) solution concept. CE-Q generalizes both Nash-Q and Friend-and-Foe-Q: in general-sum games, the set of correlated equilibria contains the set of Nash equilibria; in constant-sum games, the set of correlated equilibria contains the set of minimax equilibria. This paper describes experiments with four variants of CE-Q, demonstrating empirical convergence to equilibrium policies on a testbed of general-sum Markov games.},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{International Conference}} on {{Machine Learning}}},
  publisher = {{AAAI Press}},
  author = {Greenwald, Amy and Hall, Keith},
  year = {2003},
  pages = {242--249}
}

@incollection{LittmanMarkovgamesframework1994,
  address = {San Francisco (CA)},
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  isbn = {978-1-55860-335-6},
  abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  publisher = {{Morgan Kaufmann}},
  author = {Littman, Michael L.},
  editor = {Cohen, William W. and Hirsh, Haym},
  month = jan,
  year = {1994},
  pages = {157-163},
  file = {/Users/ranchen/Zotero/storage/YW7H6YCR/B9781558603356500271.html},
  doi = {10.1016/B978-1-55860-335-6.50027-1}
}

@inproceedings{LittmanFriendorFoeQlearningGeneralSum2001,
  address = {San Francisco, CA, USA},
  series = {ICML '01},
  title = {Friend-or-{{Foe Q}}-Learning in {{General}}-{{Sum Games}}},
  isbn = {978-1-55860-778-1},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Machine Learning}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Littman, Michael L.},
  year = {2001},
  pages = {322--328}
}

@techreport{GreenwaldCorrelatedQLearning2005,
  type = {Technical {{Report}}},
  title = {Correlated {{Q}}-{{Learning}}},
  abstract = {Recently, there have been several attempts to design multiagent learning algorithms that learn equilibrium policies in general-sum Markov games, just as Q-learning learns optimal policies in Markov decision processes. This paper introduces correlated-Q learning, one such algorithm. The contributions of this paper are twofold: (i) We show empirically that correlated-Q learns correlated equilibrium policies on a standard test bed of Markov games. (ii) We prove that certain variants of correlated-Q learning are guaranteed to converge to stationary correlated equilibrium policies in two special classes of Markov games, namely zero-sum and common-interest.},
  number = {CS-05-08},
  institution = {{Brown University}},
  author = {Greenwald, Amy and Hall, Keith and Zinkevich, Martin},
  year = {2005},
  file = {/Users/ranchen/Zotero/storage/G9R4STB3/CS-05-08.html}
}

@book{SuttonReinforcementLearningIntroduction1998,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  isbn = {978-0-262-19398-6},
  shorttitle = {Reinforcement {{Learning}}},
  abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  language = {en},
  publisher = {{MIT Press}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  keywords = {Computers / Intelligence (AI) \& Semantics}
}


